{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1cee9f",
   "metadata": {},
   "source": [
    "# last 4 layers as avg\n",
    "\n",
    "embeddings_and_labels_last_4_layers_avg.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db78bb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer # Assuming you're loading these like this elsewhere\n",
    "\n",
    "# Custom collate_fn to handle the tokenized dataset\n",
    "# The default collator works fine if you set_format(\"torch\")\n",
    "# But for clarity, we can define it.\n",
    "def collate_fn(batch):\n",
    "    # This function takes a list of individual dataset items (dictionaries)\n",
    "    # and stacks their components into tensors suitable for a batch.\n",
    "    # It ensures that input_ids, attention_mask are tensors if they aren't already\n",
    "    input_ids = torch.stack([torch.tensor(item['input_ids']) if not isinstance(item['input_ids'], torch.Tensor) else item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item['attention_mask']) if not isinstance(item['attention_mask'], torch.Tensor) else item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch]) # Labels are usually single integers, so directly convert to tensor\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def get_bert_embeddings(model, tokenizer, dataset, batch_size=32):\n",
    "    # 1. Setup and Device Placement\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    all_embeddings = [] # List to store extracted embeddings from all batches\n",
    "    all_labels = []     # List to store corresponding labels from all batches\n",
    "\n",
    "\n",
    "    # Creates an iterable over your dataset that yields batches of data\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    # 3. Inference Loop (No Gradient Calculation)\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        for batch in data_loader: # Iterate over batches provided by the DataLoader\n",
    "            # Move the batch tensors to the appropriate device (GPU/CPU)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 4. BERT Model Forward Pass for Hidden States\n",
    "            # Get model outputs. output_hidden_states=True is crucial to get all layer outputs.\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "            # outputs.hidden_states is a tuple of 13 tensors for BERT-base:\n",
    "            # - Element 0: Embedding layer output (before the first Transformer layer)\n",
    "            # - Elements 1 to 12: Outputs of the 12 Transformer encoder layers\n",
    "            # We want the last 4 layers, which represent the most abstract and contextualized representations.\n",
    "            # So, indices -1, -2, -3, -4 correspond to the last four encoder layers.\n",
    "            last_4_layers_hidden_states = outputs.hidden_states[-4:] # Tuple of 4 tensors, each (batch_size, seq_len, hidden_size)\n",
    "\n",
    "            # 5. Aggregating Across Layers (Average Pooling instead of Max Pooling)\n",
    "            # Stacks the 4 tensors along a new dimension (dimension 0).\n",
    "            # Shape changes from (4 x (batch_size, seq_len, hidden_size)) to (4, batch_size, seq_len, hidden_size)\n",
    "            stacked_hidden_states = torch.stack(last_4_layers_hidden_states)\n",
    "\n",
    "            # --- *** MODIFIED LINE HERE: Changed from torch.max to torch.mean *** ---\n",
    "            # Takes the mean value element-wise across the new 'layer' dimension (dim=0).\n",
    "            # This means for each token and each hidden dimension, it calculates the average value\n",
    "            # from its representation in the last 4 layers.\n",
    "            # Resulting shape: (batch_size, seq_len, hidden_size)\n",
    "            avg_pooled_layers = torch.mean(stacked_hidden_states, dim=0)\n",
    "            # --- *** END MODIFIED LINE *** ---\n",
    "\n",
    "\n",
    "            # 6. Aggregating Across Tokens (Attention Masking and Average Pooling)\n",
    "            # The avg_pooled_layers still contain embeddings for padding tokens (0s).\n",
    "            # We need to exclude these when averaging to get a meaningful sentence embedding.\n",
    "\n",
    "            # Expands the attention mask (batch_size, seq_len) to match the hidden_size dimension.\n",
    "            # Shape becomes (batch_size, seq_len, 1), then expanded to (batch_size, seq_len, hidden_size).\n",
    "            # This allows element-wise multiplication with the embeddings.\n",
    "            attention_mask_expanded = attention_mask.unsqueeze(-1).expand_as(avg_pooled_layers)\n",
    "\n",
    "            # Multiplies the embeddings by the expanded attention mask.\n",
    "            # This sets the embeddings of padding tokens to zero, effectively ignoring them.\n",
    "            masked_embeddings = avg_pooled_layers * attention_mask_expanded\n",
    "\n",
    "            # Sums the embeddings along the sequence length dimension (dim=1) for each sample in the batch.\n",
    "            # This gives a single vector for each sequence representing the sum of its token embeddings.\n",
    "            # Resulting shape: (batch_size, hidden_size)\n",
    "            sum_embeddings = torch.sum(masked_embeddings, dim=1)\n",
    "\n",
    "            # Counts the number of non-padding tokens in each sequence.\n",
    "            # Summing the attention mask (which contains 1s for real tokens and 0s for padding)\n",
    "            # gives the actual length of each original sequence.\n",
    "            # unsqueeze(-1) adds a dimension for broadcasting during division.\n",
    "            # Shape: (batch_size, 1)\n",
    "            num_tokens = torch.sum(attention_mask, dim=1).unsqueeze(-1)\n",
    "\n",
    "            # Prevents division by zero if an attention mask somehow ends up with zero tokens (unlikely for valid inputs).\n",
    "            num_tokens = torch.clamp(num_tokens, min=1e-9)\n",
    "\n",
    "            # Calculates the average embedding for each sequence by dividing the sum of embeddings\n",
    "            # by the number of actual tokens. This is the final document-level embedding.\n",
    "            # Resulting shape: (batch_size, hidden_size)\n",
    "            sentence_embeddings = sum_embeddings / num_tokens\n",
    "\n",
    "            # 7. Collect Results\n",
    "            # Appends the numpy representation of the batch's sentence embeddings to a list.\n",
    "            all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            # Appends the numpy representation of the batch's labels to a list.\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # 8. Final Output\n",
    "    # Stacks all collected batch embeddings vertically to form a single NumPy array.\n",
    "    # Resulting shape: (total_samples, hidden_size)\n",
    "    # Stacks all collected batch labels horizontally to form a single 1D NumPy array.\n",
    "    # Resulting shape: (total_samples,)\n",
    "    return np.vstack(all_embeddings), np.hstack(all_labels)\n",
    "\n",
    "\n",
    "# --- How the function is called ---\n",
    "\n",
    "# 9. Extracting Embeddings for Training Data\n",
    "print(\"\\nExtracting BERT embeddings for training data (avg pooling last 4 layers)...\")\n",
    "# Calls the function for your training dataset.\n",
    "# `fine_tuned_bert_embeddings_extractor` is the BERT model backbone (AutoModel)\n",
    "# that has been fine-tuned on your fake news data.\n",
    "train_embeddings, train_labels = get_bert_embeddings(bert_model, tokenizer, tokenized_datasets['train'])\n",
    "print(f\"Train Embeddings shape: {train_embeddings.shape}\") # Prints the shape of the resulting array (e.g., (N_train_samples, 768))\n",
    "print(f\"Train Labels shape: {train_labels.shape}\")       # Prints the shape of the labels array (e.g., (N_train_samples,))\n",
    "\n",
    "# 10. Extracting Embeddings for Test Data\n",
    "print(\"\\nExtracting BERT embeddings for test data (avg pooling last 4 layers)...\")\n",
    "# Calls the function similarly for your test dataset.\n",
    "# The 'validation' key here should correspond to your test split (as per your DatasetDict)\n",
    "test_embeddings, test_labels = get_bert_embeddings(bert_model, tokenizer, tokenized_datasets['validation'])\n",
    "print(f\"Test Embeddings shape: {test_embeddings.shape}\") # Prints the shape (e.g., (N_test_samples, 768))\n",
    "print(f\"Test Labels shape: {test_labels.shape}\")       # Prints the shape (e.g., (N_test_samples,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604a33c4",
   "metadata": {},
   "source": [
    "# concatenated cls tokens of last 4 layers\n",
    "\n",
    "embeddings_and_labels_concat_last_4_layers_cls.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520f54f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Assuming AutoModel and AutoTokenizer are loaded correctly, e.g.:\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# bert_model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Custom collate_fn to handle the tokenized dataset\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item['input_ids']) if not isinstance(item['input_ids'], torch.Tensor) else item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item['attention_mask']) if not isinstance(item['attention_mask'], torch.Tensor) else item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "def get_bert_embeddings_cls_concat(model, tokenizer, dataset, batch_size=32, num_last_layers=4):\n",
    "    \"\"\"\n",
    "    Extracts sentence embeddings by concatenating the [CLS] tokens\n",
    "    from the last N layers of the BERT model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "            # Get the hidden states of the specified last N layers\n",
    "            # outputs.hidden_states is a tuple where each element is (batch_size, seq_len, hidden_size)\n",
    "            # The indices -num_last_layers: will select the last N layers\n",
    "            selected_layers_hidden_states = outputs.hidden_states[-num_last_layers:]\n",
    "\n",
    "            # Extract the [CLS] token embedding (index 0) from each of these layers\n",
    "            cls_embeddings_from_layers = [\n",
    "                layer_output[:, 0, :] # Shape: (batch_size, hidden_size)\n",
    "                for layer_output in selected_layers_hidden_states\n",
    "            ]\n",
    "\n",
    "            # Concatenate these [CLS] embeddings along the feature dimension\n",
    "            # Resulting shape: (batch_size, num_last_layers * hidden_size)\n",
    "            sentence_embeddings = torch.cat(cls_embeddings_from_layers, dim=-1)\n",
    "\n",
    "            all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings), np.hstack(all_labels)\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure you have 'bert_model', 'tokenizer', and 'tokenized_datasets' loaded.\n",
    "# For example:\n",
    "# from transformers import AutoModel, AutoTokenizer\n",
    "# from datasets import DatasetDict, Dataset\n",
    "#\n",
    "# # Load a pre-trained BERT model and tokenizer (or your fine-tuned one)\n",
    "# model_name = \"bert-base-uncased\" # Or your fine-tuned model path/name\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# bert_model = AutoModel.from_pretrained(model_name) # Or AutoModelForSequenceClassification.from_pretrained(...)\n",
    "#\n",
    "# # Placeholder for your tokenized_datasets (replace with your actual data)\n",
    "# # Ensure 'input_ids', 'attention_mask', and 'labels' are present\n",
    "# tokenized_datasets = DatasetDict({\n",
    "#     'train': Dataset.from_dict({\n",
    "#         'input_ids': [[101, 2054, 2003, 102], [101, 2047, 102]],\n",
    "#         'attention_mask': [[1, 1, 1, 1], [1, 1, 1]],\n",
    "#         'labels': [0, 1]\n",
    "#     }),\n",
    "#     'validation': Dataset.from_dict({\n",
    "#         'input_ids': [[101, 2054, 2003, 102], [101, 2047, 102]],\n",
    "#         'attention_mask': [[1, 1, 1, 1], [1, 1, 1]],\n",
    "#         'labels': [0, 1]\n",
    "#     })\n",
    "# })\n",
    "\n",
    "\n",
    "print(\"\\nExtracting BERT embeddings (concatenating [CLS] tokens of last 4 layers)...\")\n",
    "train_embeddings_cls_concat, train_labels_cls_concat = get_bert_embeddings_cls_concat(bert_model, tokenizer, tokenized_datasets['train'], num_last_layers=4)\n",
    "print(f\"Train Embeddings shape: {train_embeddings_cls_concat.shape}\")\n",
    "\n",
    "print(\"\\nExtracting BERT embeddings (concatenating [CLS] tokens of last 2 layers)...\")\n",
    "test_embeddings_cls_concat_2_layers, test_labels_cls_concat_2_layers = get_bert_embeddings_cls_concat(bert_model, tokenizer, tokenized_datasets['validation'], num_last_layers=2)\n",
    "print(f\"Test Embeddings shape: {test_embeddings_cls_concat_2_layers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab665dc",
   "metadata": {},
   "source": [
    "# last 4 layers. Avg concatenated\n",
    "\n",
    "embeddings_and_labels_concat_last_4_layers_avg.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcfcfe6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Custom collate_fn to handle the tokenized dataset (same as above)\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item['input_ids']) if not isinstance(item['input_ids'], torch.Tensor) else item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item['attention_mask']) if not isinstance(item['attention_mask'], torch.Tensor) else item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def get_bert_embeddings_avg_concat(model, tokenizer, dataset, batch_size=32, num_last_layers=4):\n",
    "    \"\"\"\n",
    "    Extracts sentence embeddings by concatenating the attention-masked average pooled\n",
    "    token representations from the last N layers of the BERT model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "            selected_layers_hidden_states = outputs.hidden_states[-num_last_layers:]\n",
    "\n",
    "            layer_pooled_embeddings = []\n",
    "            for layer_output in selected_layers_hidden_states:\n",
    "                # Apply attention mask and average pooling for each layer's output\n",
    "                attention_mask_expanded = attention_mask.unsqueeze(-1).expand_as(layer_output)\n",
    "                masked_embeddings = layer_output * attention_mask_expanded\n",
    "                sum_embeddings = torch.sum(masked_embeddings, dim=1)\n",
    "                num_tokens = torch.sum(attention_mask, dim=1).unsqueeze(-1)\n",
    "                num_tokens = torch.clamp(num_tokens, min=1e-9)\n",
    "                avg_pooled_layer = sum_embeddings / num_tokens # Shape: (batch_size, hidden_size)\n",
    "                layer_pooled_embeddings.append(avg_pooled_layer)\n",
    "\n",
    "            # Concatenate these average pooled embeddings along the feature dimension\n",
    "            # Resulting shape: (batch_size, num_last_layers * hidden_size)\n",
    "            sentence_embeddings = torch.cat(layer_pooled_embeddings, dim=-1)\n",
    "\n",
    "            all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings), np.hstack(all_labels)\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Make sure you have 'bert_model', 'tokenizer', and 'tokenized_datasets' loaded.\n",
    "\n",
    "print(\"\\nExtracting BERT embeddings (concatenating average pooled last 4 layers)...\")\n",
    "train_embeddings_avg_concat, train_labels_avg_concat = get_bert_embeddings_avg_concat(bert_model, tokenizer, tokenized_datasets['train'], num_last_layers=4)\n",
    "print(f\"Train Embeddings shape: {train_embeddings_avg_concat.shape}\")\n",
    "\n",
    "print(\"\\nExtracting BERT embeddings (concatenating average pooled last 2 layers)...\")\n",
    "test_embeddings_avg_concat_2_layers, test_labels_avg_concat_2_layers = get_bert_embeddings_avg_concat(bert_model, tokenizer, tokenized_datasets['validation'], num_last_layers=2)\n",
    "print(f\"Test Embeddings shape: {test_embeddings_avg_concat_2_layers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ac67d",
   "metadata": {},
   "source": [
    "# cls token of last layer\n",
    "\n",
    "embeddings_and_labels_cls_last_layer.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751b499",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer # Assuming you're loading these like this elsewhere\n",
    "\n",
    "# Custom collate_fn to handle the tokenized dataset\n",
    "# The default collator works fine if you set_format(\"torch\")\n",
    "# But for clarity, we can define it.\n",
    "def collate_fn(batch):\n",
    "    # This function takes a list of individual dataset items (dictionaries)\n",
    "    # and stacks their components into tensors suitable for a batch.\n",
    "    # It ensures that input_ids, attention_mask are tensors if they aren't already\n",
    "    input_ids = torch.stack([torch.tensor(item['input_ids']) if not isinstance(item['input_ids'], torch.Tensor) else item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item['attention_mask']) if not isinstance(item['attention_mask'], torch.Tensor) else item['attention_mask'] for item in batch])\n",
    "    labels = torch.tensor([item['labels'] for item in batch]) # Labels are usually single integers, so directly convert to tensor\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "def get_bert_embeddings(model, tokenizer, dataset, batch_size=32):\n",
    "    # 1. Setup and Device Placement\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    all_embeddings = [] # List to store extracted embeddings from all batches\n",
    "    all_labels = []     # List to store corresponding labels from all batches\n",
    "\n",
    "\n",
    "    # Creates an iterable over your dataset that yields batches of data\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    # 3. Inference Loop (No Gradient Calculation)\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        for batch in data_loader: # Iterate over batches provided by the DataLoader\n",
    "            # Move the batch tensors to the appropriate device (GPU/CPU)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # 4. BERT Model Forward Pass for Hidden States\n",
    "            # Get model outputs. output_hidden_states=True is crucial to get all layer outputs.\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "\n",
    "            # --- *** MODIFIED SECTION: Extracting CLS Token Embedding *** ---\n",
    "            # outputs.last_hidden_state is the output of the final BERT layer.\n",
    "            # Its shape is (batch_size, sequence_length, hidden_size).\n",
    "            # The [CLS] token is always the first token (index 0) in the sequence.\n",
    "            # So, outputs.last_hidden_state[:, 0, :] extracts the embedding\n",
    "            # for the [CLS] token for all samples in the current batch.\n",
    "            sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            # --- *** END MODIFIED SECTION *** ---\n",
    "\n",
    "            # 7. Collect Results\n",
    "            # Appends the numpy representation of the batch's sentence embeddings to a list.\n",
    "            all_embeddings.append(sentence_embeddings.cpu().numpy())\n",
    "            # Appends the numpy representation of the batch's labels to a list.\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "    # 8. Final Output\n",
    "    # Stacks all collected batch embeddings vertically to form a single NumPy array.\n",
    "    # Resulting shape: (total_samples, hidden_size)\n",
    "    # Stacks all collected batch labels horizontally to form a single 1D NumPy array.\n",
    "    # Resulting shape: (total_samples,)\n",
    "    return np.vstack(all_embeddings), np.hstack(all_labels)\n",
    "\n",
    "# --- How the function is called (remains the same) ---\n",
    "# Assuming 'bert_model' and 'tokenizer' are already defined and loaded,\n",
    "# and 'tokenized_datasets' is your dataset dictionary.\n",
    "\n",
    "# Example placeholder for your model and tokenizer (ensure these are loaded as they were before)\n",
    "# from transformers import BertForSequenceClassification # if you fine-tuned a classification head\n",
    "# model_name = \"bert-base-uncased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# bert_model = AutoModel.from_pretrained(model_name) # or BertForSequenceClassification if that's what you fine-tuned\n",
    "\n",
    "# Make sure tokenized_datasets is defined, e.g.:\n",
    "# from datasets import DatasetDict, Dataset\n",
    "# tokenized_datasets = DatasetDict({\n",
    "#     'train': Dataset.from_dict({'input_ids': ..., 'attention_mask': ..., 'labels': ...}),\n",
    "#     'validation': Dataset.from_dict({'input_ids': ..., 'attention_mask': ..., 'labels': ...})\n",
    "# })\n",
    "\n",
    "\n",
    "# 9. Extracting Embeddings for Training Data\n",
    "print(\"\\nExtracting BERT embeddings for training data (using [CLS] token)...\")\n",
    "train_embeddings, train_labels = get_bert_embeddings(bert_model, tokenizer, tokenized_datasets['train'])\n",
    "print(f\"Train Embeddings shape: {train_embeddings.shape}\")\n",
    "print(f\"Train Labels shape: {train_labels.shape}\")\n",
    "\n",
    "# 10. Extracting Embeddings for Test Data\n",
    "print(\"\\nExtracting BERT embeddings for test data (using [CLS] token)...\")\n",
    "test_embeddings, test_labels = get_bert_embeddings(bert_model, tokenizer, tokenized_datasets['validation'])\n",
    "print(f\"Test Embeddings shape: {test_embeddings.shape}\")\n",
    "print(f\"Test Labels shape: {test_labels.shape}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
