{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42244b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m model = BertForSequenceClassification.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m                                                       num_labels=\u001b[32m2\u001b[39m,\n\u001b[32m      5\u001b[39m                                                      output_attentions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      6\u001b[39m                                                      output_hidden_states=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m\"\u001b[39m\u001b[33mbert-base-uncased\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/modeling_utils.py:3807\u001b[39m, in \u001b[36mPreTrainedModel.cuda\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3802\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3803\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3804\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3805\u001b[39m         )\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1065\u001b[39m, in \u001b[36mModule.cuda\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n\u001b[32m   1049\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[32m   1050\u001b[39m \n\u001b[32m   1051\u001b[39m \u001b[33;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1063\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    914\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m915\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    918\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    919\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    920\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    926\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:942\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    939\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    940\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    945\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1065\u001b[39m, in \u001b[36mModule.cuda.<locals>.<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] = \u001b[38;5;28;01mNone\u001b[39;00m) -> T:\n\u001b[32m   1049\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[32m   1050\u001b[39m \n\u001b[32m   1051\u001b[39m \u001b[33;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1063\u001b[39m \u001b[33;03m        Module: self\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1065\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/cuda/__init__.py:363\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    359\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    361\u001b[39m     )\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    366\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=2,\n",
    "                                                     output_attentions=True,\n",
    "                                                     output_hidden_states=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d25768d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line of the dataset: Rot-Weiss Essen besiegt Zweitligist Düsseldorf und zieht als einziger Viertligist ins Achtelfinale ein. Wolfsburg, Stuttgart und Bremen gewinnen. Der frühere Bundesligist Rot-Weiss Essen steht nach der Überraschung im Nachbarschafts-Duell mit Fortuna Düsseldorf erstmals seit zwölf Jahren wieder im DFB-Pokal-Achtelfinale. Der bislang noch ungeschlagene Spitzenreiter der Regionalliga West kam am Mittwoch zu einem glücklichen 3:2 (2:1) in der zweiten Pokalrunde gegen den Absteiger aus der Fußball-Bundesliga. Simon Engelmann (15. Minute), Kapitän Marco Kehl-Gomez (39.) und der eingewechselte Oguzhan Kefkir (70.) nutzten die einzigen drei echten Chancen des Viertligisten eiskalt aus.\n",
      "Die deutlich feldüberlegene Fortuna kam trotz ihres höheren Aufwands nur zum zwischenzeitlichen 1:1 und zum späten Elfmeter-Tor jeweils durch Torjäger Rouwen Hennings (36./87.). Damit scheiterte das Team von Trainer Uwe Rösler wie schon in der Vorsaison an einem Viertligisten. Im März hatte die Fortuna als Bundesligist das Viertelfinale beim 1. FC Saarbrücken nach Elfmeterschießen verloren.\n",
      "In Essen war die Fortuna von Beginn an spielerisch stärker, tat sich aber vor dem Tor schwer. Ganz anders die selbstbewussten Essener, die in 20 Saisonspielen noch ohne Niederlage sind. Das Team von Trainer Christian Neidhart hatte zudem in der ersten Pokalrunde bereits Bundesligist Arminia Bielefeld mit 1:0 ausgeschaltet. Auch am Mittwoch lief fast alles für den Traditionsclub aus dem Revier, der zuletzt vor 43 Jahren in der Bundesliga gespielt hatte.\n",
      "Torjäger Engelmann nutzte früh einen üblen Patzer von Düsseldorfs Ersatzkeeper Raphael Wolf aus. Nur drei Minuten nach Hennings' hochverdientem Ausgleich schlug RWE zurück: Kehl-Gomez nutzte ein Abstimmungsproblem in der Fortuna-Abwehr aus und wuchtete den Ball per Kopf ins Netz. Im zweiten Durchgang mühte sich die Fortuna so umständlich wie vergeblich. Ein Konter des deutschen Meisters von 1955 genügte für die Entscheidung. Mehr als ein verwandelter Foulelfmeter durch Hennings war für die Gäste nicht mehr drin.\n",
      "Anders als Düsseldorf setzte sich der Vfl Wolfsburg souverän durch. Der Tabellenvierte der Fußball-Bundesliga besiegte den SV Sandhausen trotz Corona-Sorgen und zahlreicher Ausfälle hochverdient mit 4:0 (3:0). Yannick Gerhardt in der 27. Minute, Wout Weghorst (30./90.+1) und João Victor (41.) erzielten am Mittwochabend die Tore für die in allen Belangen überlegene Mannschaft von Trainer Oliver Glasner. Zweitliga-Abstiegskandidat Sandhausen konnte nicht mithalten und war im Angriff völlig harmlos.\n",
      "Der VfB Stuttgart hat mit einem 1:0 (1:0)-Sieg gegen den Bundesliga-Rivalen SC Freiburg den Einzug ins Achtelfinale geschafft. Auch Werder Bremen, das 3:0 (2:0) bei Hannover 96 gewann, überstand die zweite Runde. (dpa)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../datasets/combined/combined_dataset_200samples_50-50split.csv')\n",
    "first_article = df.iloc[0]['article']\n",
    "\n",
    "print(f\"First line of the dataset: {first_article}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ed05e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original first article: Rot-Weiss Essen besiegt Zweitligist Düsseldorf und zieht als einziger Viertligist ins Achtelfinale ein. Wolfsburg, Stuttgart und Bremen gewinnen. Der frühere Bundesligist Rot-Weiss Essen steht nach der Überraschung im Nachbarschafts-Duell mit Fortuna Düsseldorf erstmals seit zwölf Jahren wieder im DFB-Pokal-Achtelfinale. Der bislang noch ungeschlagene Spitzenreiter der Regionalliga West kam am Mittwoch zu einem glücklichen 3:2 (2:1) in der zweiten Pokalrunde gegen den Absteiger aus der Fußball-Bundesliga. Simon Engelmann (15. Minute), Kapitän Marco Kehl-Gomez (39.) und der eingewechselte Oguzhan Kefkir (70.) nutzten die einzigen drei echten Chancen des Viertligisten eiskalt aus.\n",
      "Die deutlich feldüberlegene Fortuna kam trotz ihres höheren Aufwands nur zum zwischenzeitlichen 1:1 und zum späten Elfmeter-Tor jeweils durch Torjäger Rouwen Hennings (36./87.). Damit scheiterte das Team von Trainer Uwe Rösler wie schon in der Vorsaison an einem Viertligisten. Im März hatte die Fortuna als Bundesligist das Viertelfinale beim 1. FC Saarbrücken nach Elfmeterschießen verloren.\n",
      "In Essen war die Fortuna von Beginn an spielerisch stärker, tat sich aber vor dem Tor schwer. Ganz anders die selbstbewussten Essener, die in 20 Saisonspielen noch ohne Niederlage sind. Das Team von Trainer Christian Neidhart hatte zudem in der ersten Pokalrunde bereits Bundesligist Arminia Bielefeld mit 1:0 ausgeschaltet. Auch am Mittwoch lief fast alles für den Traditionsclub aus dem Revier, der zuletzt vor 43 Jahren in der Bundesliga gespielt hatte.\n",
      "Torjäger Engelmann nutzte früh einen üblen Patzer von Düsseldorfs Ersatzkeeper Raphael Wolf aus. Nur drei Minuten nach Hennings' hochverdientem Ausgleich schlug RWE zurück: Kehl-Gomez nutzte ein Abstimmungsproblem in der Fortuna-Abwehr aus und wuchtete den Ball per Kopf ins Netz. Im zweiten Durchgang mühte sich die Fortuna so umständlich wie vergeblich. Ein Konter des deutschen Meisters von 1955 genügte für die Entscheidung. Mehr als ein verwandelter Foulelfmeter durch Hennings war für die Gäste nicht mehr drin.\n",
      "Anders als Düsseldorf setzte sich der Vfl Wolfsburg souverän durch. Der Tabellenvierte der Fußball-Bundesliga besiegte den SV Sandhausen trotz Corona-Sorgen und zahlreicher Ausfälle hochverdient mit 4:0 (3:0). Yannick Gerhardt in der 27. Minute, Wout Weghorst (30./90.+1) und João Victor (41.) erzielten am Mittwochabend die Tore für die in allen Belangen überlegene Mannschaft von Trainer Oliver Glasner. Zweitliga-Abstiegskandidat Sandhausen konnte nicht mithalten und war im Angriff völlig harmlos.\n",
      "Der VfB Stuttgart hat mit einem 1:0 (1:0)-Sieg gegen den Bundesliga-Rivalen SC Freiburg den Einzug ins Achtelfinale geschafft. Auch Werder Bremen, das 3:0 (2:0) bei Hannover 96 gewann, überstand die zweite Runde. (dpa)\n",
      "Tokenized first article: [[  101   155  3329   118 19375   142 12789  1129  9819 19815   163 24078\n",
      "  26414 24874  2050 19822  5576   195  1663 17439  2393  1116   174  1394\n",
      "  24461  1200   159  2852 26414 24874  2050 22233   138  9817 22775 14196\n",
      "   1162   174  1394   119  6499  9364   117 13269  5576 17339   176  5773\n",
      "  25409  1424   119  9682   175  1197 17176 12807   139  6775  1279  2646\n",
      "  25019  1204   155  3329   118 19375   142 12789   188  1566 17439  9468\n",
      "   1732  4167   243  3169  7297 17143  2118 13280 11896  1732  6824  9022\n",
      "  17495  1116   118  4187  2339 26410  3144  9291 19822 14044  2050  7435\n",
      "   1116 14516  2875   195  2246 19593  9654   147  3354  5123   192  4830\n",
      "   1200 13280   141 24366   118 18959 14046   118   138  9817 22775 14196\n",
      "   1162   119  9682 16516 26597  2118  1185  1732  8362  7562  1732 19640\n",
      "   1673   156 18965 10947  1874 19385  4167  4723 10811  1537 24181  1306\n",
      "   1821 12107  3069 12821  1732 21458   174  2042  1306   176  1233 24116\n",
      "  26567  1179   124   131   123   113   123   131   122   114  1107  4167\n",
      "    195 24078  5208 18959 14046 10607  2007   176 27487  1179 10552   138\n",
      "   4832  1566 22896 12686  1116  4167 14763 21426  5892   118 14814   119\n",
      "   3274 13832  8863  4119   113  1405   119 14321  6140   114   117 14812\n",
      "  18965 17479  1179  8571 26835  8495   118 21411   113  3614   119   114\n",
      "   5576  4167   174 27048  7921 17704  1883  1566   152 13830  1584  3822\n",
      "  26835  2087  2293  1197   113  3102   119   114 22664  1584  5208  2939\n",
      "    174  1394 24461  1424   173  1874  1182   174  9817  1424 12423  1179\n",
      "   3532   159  2852 26414 24874 15874   174 13189  1348  1204 12686  1116\n",
      "    119  5736  1260  3818 16879   175 22654 17176  3169 27412  7582  3144\n",
      "   9291 24181  1306   189 10595  1584   178  8167  1279   177 19593 12807\n",
      "   1179 27758  2087  5491  3680   183  2149 21458  1306   195 10073 24169\n",
      "   3171  2875 26567  1179   122   131   122  5576 21458  1306   188  1643\n",
      "  27888  1424  2896  2087 19401   118 19928   179  5773 13593  1116  3840\n",
      "  10340 19928  3361 17479  2895   155  6094 10781  1124 15918  1116   113\n",
      "   3164   119   120  5966   119   114   119  8732  2875   188  4386 19385\n",
      "   1566 19875  2649  3262  9791  1200   158  7921   155 19593 21318   192\n",
      "   1663   188  8401  1179  1107  4167   159  3864 15837  1320  1126   174\n",
      "   2042  1306   159  2852 26414 24874 15874   119   146  1306   150 17479\n",
      "  11819  6131  1566  2939  3144  9291  2393  1116   139  6775  1279  2646\n",
      "  25019  1204 19875   159  2852  7854 25154  1162  1129  4060   122   119\n",
      "   3604 17784  1813  1830  1197 24116  1424  9468  1732  2896  2087 19401\n",
      "   9022  1663 13750  1179  1396 17670  4474  1179   119  1130   142 12789\n",
      "   1594  2939  3144  9291  3262  4108 10533  1179  1126   188 15418  2879\n",
      "  16253   188  1204 17479  4661  1200   117 27629  1204 16554  1324   170\n",
      "   3169   191  1766 26707 19928   188  1732 12097   119   144  1389  1584\n",
      "   1105  1468  2939 14516  1233  4832  1204  3962 20236  3954  5208   142\n",
      "  12789  1200   117  2939  1107  1406 25185 22966 15418  7836  1185  1732\n",
      "   9294  1673 27453 15018 17670  2553 11850  1181   119 10730  2649  3262\n",
      "   9791  1200  2131   151  6851 14016  3740   102]]\n",
      "Length of original first article: 2777\n",
      "Length of tokenized first article: 1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "encoded_dict = tokenizer.encode_plus(\n",
    "    first_article,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    return_attention_mask=True, # set 0 if token is not present (padding), set 1 if token is present\n",
    "    return_tensors='tf',\n",
    "    padding='max_length'  # Ensure all sequences are of the same length\n",
    ")\n",
    "\n",
    "encoded_dict['input_ids']\n",
    "encoded_dict['attention_mask']\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Original first article: {first_article}\")\n",
    "print(f\"Tokenized first article: {encoded_dict['input_ids']}\")\n",
    "\n",
    "print(f\"Length of original first article: {len(first_article)}\")\n",
    "print(f\"Length of tokenized first article: {len(encoded_dict['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f742d87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(encoded_dict['input_ids'], attention_mask=encoded_dict['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92b697f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.48230153,  0.32629097,  0.37618014, ...,  0.28631747,\n",
       "          -0.90260947,  0.01283996],\n",
       "         [-1.9574678 ,  0.11998283, -0.52765363, ..., -1.0067387 ,\n",
       "          -0.75383043, -0.2297786 ],\n",
       "         [-1.3107111 ,  2.3562386 ,  1.7727066 , ...,  0.44063115,\n",
       "           0.30648267, -0.75741875],\n",
       "         ...,\n",
       "         [ 1.9676633 , -0.23459473,  1.3176453 , ..., -0.19063394,\n",
       "          -0.9960441 , -0.3939369 ],\n",
       "         [-0.32624888,  0.31903368,  1.6460272 , ...,  0.36437565,\n",
       "           0.18716516,  0.39599824],\n",
       "         [-0.51195425,  0.46750206,  0.5430804 , ...,  0.03593849,\n",
       "          -1.0480646 , -1.6619052 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.28608802,  0.4927413 ,  0.20727777, ...,  0.4563554 ,\n",
       "          -0.48655015,  0.10393047],\n",
       "         [-1.5829247 ,  0.6943203 , -0.10037682, ..., -0.96523136,\n",
       "          -0.77860594, -0.012145  ],\n",
       "         [-1.0254295 ,  2.3398354 ,  2.1005626 , ...,  0.25104818,\n",
       "           0.8531209 , -0.62303   ],\n",
       "         ...,\n",
       "         [ 1.9207336 ,  0.37123433,  1.075484  , ..., -0.01562748,\n",
       "          -1.1319984 , -0.07748641],\n",
       "         [-0.7593642 ,  0.66652393,  1.0872306 , ...,  0.94657993,\n",
       "           0.15105745,  0.5783485 ],\n",
       "         [-0.48490688,  0.5246676 ,  0.53941166, ...,  0.48212993,\n",
       "          -1.2546608 , -1.5037743 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.12345058,  0.37522346,  0.12134699, ...,  0.55404973,\n",
       "          -0.7230389 ,  0.34310707],\n",
       "         [-1.5701243 ,  0.8726877 ,  0.12420941, ..., -0.63822997,\n",
       "          -0.9986752 , -0.06690636],\n",
       "         [-0.22141413,  1.6901059 ,  2.2396457 , ..., -0.33551705,\n",
       "           0.975566  , -0.17928827],\n",
       "         ...,\n",
       "         [ 1.9070464 ,  0.7045092 ,  0.85998076, ...,  0.24466482,\n",
       "          -1.0205175 , -0.07231139],\n",
       "         [-0.43731546,  0.5554139 ,  0.98410237, ...,  1.0659668 ,\n",
       "           0.2596183 ,  0.1848515 ],\n",
       "         [-0.26054117,  0.54752845, -0.08301342, ...,  1.2133275 ,\n",
       "          -1.1138443 , -1.5714544 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.17185086,  0.31414264,  0.3795498 , ...,  0.61843884,\n",
       "          -0.50671834,  0.0309145 ],\n",
       "         [-1.2454648 ,  1.0581465 ,  0.41574433, ..., -0.8847691 ,\n",
       "          -0.8555132 , -0.10036017],\n",
       "         [-0.41637617,  2.016087  ,  2.3033478 , ..., -0.9310387 ,\n",
       "           0.7797404 ,  0.3798601 ],\n",
       "         ...,\n",
       "         [ 1.7011161 ,  0.9861178 ,  1.3387649 , ...,  0.6766441 ,\n",
       "          -1.205657  , -0.17759843],\n",
       "         [ 0.29548654,  0.9899485 ,  0.9320308 , ...,  1.513945  ,\n",
       "           0.364979  ,  0.20033744],\n",
       "         [-0.3137882 ,  0.32473412,  0.04833745, ...,  1.5496781 ,\n",
       "          -1.1621948 , -1.7039456 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[ 0.02453339,  0.35345387,  0.88794327, ...,  0.48604697,\n",
       "          -0.88161427, -0.00418808],\n",
       "         [-1.9518723 ,  0.9834359 ,  0.742743  , ..., -0.418795  ,\n",
       "          -0.75135016,  0.12593511],\n",
       "         [-0.45919743,  2.1516314 ,  2.2630892 , ..., -0.9873209 ,\n",
       "           0.44147786,  0.20476988],\n",
       "         ...,\n",
       "         [ 1.9212575 ,  0.9277555 ,  0.9545697 , ...,  0.9349249 ,\n",
       "          -1.4379528 , -0.05726372],\n",
       "         [ 0.34505033,  1.3758954 ,  0.6962715 , ...,  1.6594732 ,\n",
       "          -0.15043849, -0.07550919],\n",
       "         [-0.08200823,  0.2664445 , -0.23847523, ...,  1.2010825 ,\n",
       "          -1.1431501 , -1.2142599 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.2763708 ,  0.2842126 ,  0.9210852 , ..., -0.06177637,\n",
       "          -0.7770796 , -0.34292775],\n",
       "         [-2.5722103 ,  1.2378539 ,  0.8115642 , ..., -0.33548653,\n",
       "          -0.6373975 , -0.4993617 ],\n",
       "         [-0.6383526 ,  2.2742927 ,  2.2348158 , ..., -1.3841627 ,\n",
       "           0.37696815, -0.69078624],\n",
       "         ...,\n",
       "         [ 1.932948  ,  0.973284  ,  1.0194939 , ...,  0.14297614,\n",
       "          -1.4962472 , -0.16350324],\n",
       "         [ 0.18247841,  1.1908957 ,  0.7104007 , ...,  1.3383619 ,\n",
       "          -0.2604999 , -0.8212614 ],\n",
       "         [-0.454686  ,  0.24259722,  0.11147944, ...,  0.7417334 ,\n",
       "          -1.2691743 , -1.3789134 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[ 0.01886767, -0.45231786,  0.2553184 , ...,  0.6913993 ,\n",
       "          -0.9661693 , -0.16639648],\n",
       "         [-2.3373876 ,  0.74534833,  0.37825248, ...,  0.4423574 ,\n",
       "          -1.3541806 , -0.00986049],\n",
       "         [-0.919287  ,  1.2592086 ,  0.8517667 , ..., -0.64775896,\n",
       "          -0.26741385,  0.25998327],\n",
       "         ...,\n",
       "         [ 1.5664959 ,  0.09353755,  0.4337542 , ...,  0.64244485,\n",
       "          -1.7031549 ,  0.3572425 ],\n",
       "         [ 0.10549907, -0.16385545,  0.3992617 , ...,  1.5489388 ,\n",
       "          -0.91380376, -0.35482538],\n",
       "         [-1.0233204 , -0.19770844,  0.02204936, ...,  0.8834227 ,\n",
       "          -1.6204829 , -1.1090705 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.21080042, -1.0239667 ,  0.22610034, ...,  1.0450798 ,\n",
       "          -0.5115431 , -0.04727335],\n",
       "         [-1.8853178 ,  0.28393322,  0.45707187, ...,  0.2635567 ,\n",
       "          -1.1826444 ,  0.04918563],\n",
       "         [-0.78235865,  0.3747437 ,  1.0158075 , ..., -0.3740982 ,\n",
       "           0.09546124,  0.19245152],\n",
       "         ...,\n",
       "         [ 2.1455865 , -0.33714595,  0.8690281 , ...,  0.6543481 ,\n",
       "          -1.5724589 , -0.59047693],\n",
       "         [ 0.8556604 , -0.7095245 ,  0.7783103 , ...,  1.792284  ,\n",
       "          -1.1184751 , -1.0453303 ],\n",
       "         [-0.4607922 , -0.69674253,  0.05986277, ...,  0.55404955,\n",
       "          -1.5985857 , -1.0917488 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.2245004 , -1.1151388 , -0.20643765, ...,  1.3692855 ,\n",
       "          -0.70754474, -0.37668064],\n",
       "         [-1.7879548 ,  0.22645423,  0.5799593 , ...,  0.30595782,\n",
       "          -1.3656752 , -0.36281824],\n",
       "         [-0.603795  ,  0.46262538,  1.2782323 , ..., -0.04568732,\n",
       "           0.19025618, -0.47100732],\n",
       "         ...,\n",
       "         [ 1.9093276 , -0.26676893,  1.0575684 , ...,  0.8851499 ,\n",
       "          -1.2144283 , -0.5910083 ],\n",
       "         [ 1.3280103 , -0.21401845,  1.1559937 , ...,  1.6917716 ,\n",
       "          -0.6794394 , -1.1305317 ],\n",
       "         [-0.03985101, -0.17499842,  0.366429  , ...,  1.0635171 ,\n",
       "          -0.8331965 , -1.397198  ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[ 0.03920549, -1.1126012 , -0.42958817, ...,  1.4418275 ,\n",
       "          -0.78333765, -0.52074796],\n",
       "         [-1.4974321 ,  1.3301843 ,  0.39026743, ...,  0.05645818,\n",
       "          -0.65063125, -0.2536888 ],\n",
       "         [-0.8503869 ,  0.9454019 ,  1.6113398 , ...,  0.37327   ,\n",
       "           0.2594611 , -0.69762725],\n",
       "         ...,\n",
       "         [ 1.3853767 , -0.8317238 ,  0.9716406 , ...,  0.37010357,\n",
       "          -0.5135702 , -0.6280223 ],\n",
       "         [ 1.2544233 , -0.38797864,  1.0094254 , ...,  1.8315611 ,\n",
       "          -0.45204264, -1.1621033 ],\n",
       "         [ 0.09274446, -0.35450476,  0.34267843, ...,  0.66603065,\n",
       "          -0.6032036 , -1.2121103 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.01341451, -0.6331948 , -0.09756197, ...,  1.8500915 ,\n",
       "          -1.2494267 ,  0.08029876],\n",
       "         [-0.98713136,  1.197152  ,  0.8119314 , ...,  0.2849516 ,\n",
       "          -0.4912409 ,  0.2374439 ],\n",
       "         [-0.1170513 ,  1.198879  ,  1.9552907 , ...,  0.16213933,\n",
       "           0.28389424,  0.14231062],\n",
       "         ...,\n",
       "         [ 0.8241136 , -0.4672028 ,  1.3466327 , ...,  1.043014  ,\n",
       "          -0.70783263, -0.14366283],\n",
       "         [ 0.98157746, -0.00824681,  1.028849  , ...,  2.4334655 ,\n",
       "          -1.1520842 , -0.6998914 ],\n",
       "         [ 0.18192181,  0.1636634 ,  0.34278736, ...,  1.2306721 ,\n",
       "          -1.1990632 , -0.772145  ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-0.90082616,  0.66961193,  0.17968558, ...,  1.7080044 ,\n",
       "          -1.3557365 ,  0.5860086 ],\n",
       "         [-1.3229301 ,  1.83307   ,  0.59520495, ...,  0.7858558 ,\n",
       "          -0.3494913 ,  0.66010153],\n",
       "         [-0.3884347 ,  1.8172224 ,  2.5495818 , ...,  0.44111335,\n",
       "           0.85425264,  0.35955092],\n",
       "         ...,\n",
       "         [ 0.31129882,  0.227222  ,  1.6029198 , ...,  1.2633169 ,\n",
       "          -0.8606137 ,  0.5990717 ],\n",
       "         [ 0.7827371 ,  1.4813203 ,  0.93208295, ...,  2.5571003 ,\n",
       "          -0.9341339 ,  0.20780735],\n",
       "         [-0.34829414,  0.9601699 ,  0.26438063, ...,  1.3732935 ,\n",
       "          -0.9701425 , -0.09398194]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 512, 768), dtype=float32, numpy=\n",
       " array([[[-1.2061764 ,  1.6360426 ,  0.62464184, ...,  1.3789603 ,\n",
       "          -0.19438878,  0.38428664],\n",
       "         [-1.7947497 ,  2.2275765 ,  0.9338291 , ...,  1.0726479 ,\n",
       "           0.27215573,  0.17836747],\n",
       "         [-1.1150285 ,  1.9735857 ,  2.0924795 , ...,  0.6942899 ,\n",
       "           0.84458375,  0.1102029 ],\n",
       "         ...,\n",
       "         [-0.37126216,  1.3445725 ,  1.3547351 , ...,  1.1085814 ,\n",
       "           0.49155992,  0.612886  ],\n",
       "         [-0.65449286,  2.395558  ,  0.8391268 , ...,  1.791418  ,\n",
       "           0.05536737,  0.13428341],\n",
       "         [-1.3164394 ,  1.8922327 ,  0.74925697, ...,  1.283584  ,\n",
       "           0.2001981 , -0.04138603]]], dtype=float32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
