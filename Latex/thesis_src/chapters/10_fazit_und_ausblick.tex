\chapter{Fazit}
\label{chap:fazit}

Ziel dieser Arbeit war die Konzeption und prototypische Umsetzung einer Anwendung zur automatisierten Erkennung von Falschmeldungen in Online-Nachrichtenartikeln. 
Aufbauend auf aktuellen gesellschaftlichen Herausforderungen im Umgang mit Fake News wurde ein Prototyp konzipiert, 
implementiert und evaluiert, der moderne Methoden des Natural Language Processing (NLP) mit klassischem maschinellen Lernen kombiniert.

Im Mittelpunkt der Arbeit standen zwei Modellierungsansätze: Einerseits wurden vortrainierte Transformer-Modelle (BERT, RoBERTa, XLM-RoBERTa) 
direkt per Fine-Tuning für die binäre Klassifikation optimiert. Andererseits wurden Embeddings dieser Modelle extrahiert und als Feature-Vektor 
für LightGBM-Modelle verwendet. Ergänzend dazu wurde ein eigener deutschsprachiger Datensatz zusammengestellt, um die Modelle praxisnah zu trainieren und zu testen.
Zur Umsetzung kamen aktuelle Frameworks wie Python, Hugging Face Transformers sowie PyTorch zum Einsatz.

Die Evaluation zeigte, dass insbesondere das Modell XLM-RoBERTa-Large im direkten Fine-Tuning mit einer Accuracy und einem F1-Score von jeweils 97,95\% 
die besten Resultate erzielte. Auch im hybriden Ansatz mit LightGBM auf Basis von Transformer-Embeddings erreichte dieses Modell hohe Werte (Accuracy: 97,84\%). 
Über alle Experimente hinweg zeigte sich, dass die \textit{Large}-Varianten der Modelle tendenziell bessere Resultate liefern als die \textit{Base}-Versionen. 
Allerdings mit erhöhtem Rechenaufwand.

Entgegen der ursprünglichen Annahme, dass die Kombination aus Transformer-Embeddings und LightGBM-Classifiern eine bessere Generalisierungsleistung
ermöglichen könnte, schnitten die hybriden Modelle insgesamt leicht schlechter ab als die direkt feinjustierten Transformer. 
Diese Beobachtung legt nahe, dass das direkte Fine-Tuning – trotz potenziell höherer Rechenkosten – die leistungsstärkere Strategie für die Fake-News-Klassifikation 
darstellt. Der Vergleich mit verwandten Studien unterstreicht die Wettbewerbsfähigkeit der entwickelten Modelle, auch wenn unterschiedliche Datensätze und 
Modellkonfigurationen einen direkten Leistungsvergleich erschweren.

Mit dem entwickelten funktionsfähigen Prototypen wurde ein wichtiges Ziel der Arbeit erreicht: 
Eine praxistaugliche Anwendung, die durch automatisierte Klassifikation Fake News erkennen kann.

Dennoch wurde im Verlauf der Arbeit deutlich, dass insbesondere im Kontext der Fake-News-Erkennung ein breit gefächerter und aktueller Trainingsdatensatz essenziell ist.
Nur durch eine thematisch vielfältige und zeitnahe Datenbasis kann das Modell ausreichend kontextuelles Wissen aufbauen, um Nachrichteninhalte zuverlässig und differenziert 
klassifizieren zu können.

Insgesamt beweist diese Arbeit, dass transformerbasierte Modelle alleinstehend oder in Kombination mit modernen Frameworks eine effektive Grundlage 
für die automatisierte Fake-News-Erkennung sein können.

\chapter{Ausblick}
\label{chap:ausblick}

Die in dieser Arbeit entwickelte Anwendung bildet eine solide Grundlage für die automatisierte Klassifikation von Nachrichtenartikeln. 
Gleichzeitig ergeben sich aus den gewonnenen Erkenntnissen viele Ansatzpunkte für eine weiterführende Forschung und technische Weiterentwicklung.

Ein zentraler Aspekt für zukünftige Arbeiten ist der Einsatz eines aktuelleren, domänenübergreifenden Datensatzes, der verschiedene Themengebiete abdeckt 
und damit eine verbesserte Generalisierungsfähigkeit der Modelle ermöglicht. Der in dieser Arbeit verwendete, eigens zusammengestellte Datensatz 
beinhaltet einen starken Fokus auf den COVID-19 Pandemie Kontext und ist außerdem stark veraltet. 
Eine Ausweitung auf weitere thematische und aktuellere Domänen könnte die Robustheit der Modelle signifikant erhöhen.

Des Weiteren bietet sich der Einsatz leistungsfähigerer Transformer-Modelle wie XLM-RoBERTa-XL an. Dieses Modell stellt eine noch größere Variante des in 
dieser Arbeit bereits erfolgreich getesteten XLM-RoBERTa-Large dar und könnte insbesondere bei umfangreicheren Texten eine noch tiefere semantische 
Repräsentation ermöglichen. Gescheitert ist die Verwendung dieses Modells an nicht ausreichenden Hardwaremöglichkeiten.

Ein bedeutender Limitationsfaktor vieler aktueller Transformer-Modelle ist die Beschränkung auf eine maximale Inputlänge von 512 Tokens. 
Künftig könnten entweder Modelle mit erweiterter Kontextlänge (z.B. Longformer oder BigBird) oder Strategien zur Input-Segmentierung, 
wie die Kombination von Head- und Tail-Abschnitten eines Dokuments \cite{sun2020finetuneberttextclassification}, eingesetzt werden. 
Auf diese Weise ließen sich längere Nachrichtenartikel ohne Informationsverlust verarbeiten.

Auch auf der Ebene der Repräsentationserzeugung aus den Transformer-Modellen bieten sich Optimierungsmöglichkeiten. 
In dieser Arbeit wurden die Mittelwerte der letzten vier Hidden Layers genutzt, doch es existieren weitere Alternativen wie z.B. die Verwendung des 
\texttt{[CLS]}-Tokens, der maximale Layer-Wert, oder unterschiedliche Gewichtungen der Layer-Repräsentationen. 
Ein systematischer Vergleich dieser Methoden könnte zu einer verbesserten Repräsentationsqualität führen.

Darüber hinaus wäre ein Vergleich mit alternativen Modellarchitekturen auf Deep-Learning-Basis von Interesse – etwa dem Ansatz FakeBERT \cite{Kaliyar:2021aa}, 
welcher CNNs mit BERT kombiniert. Besonders vielversprechend erscheint die Kombination von FakeBERT mit XLM-RoBERTa als Embedding-Modell,
um die semantische Tiefe transformerbasierter Repräsentationen mit den extraktiven Fähigkeiten von Convolutional Layers zu verbinden.
Generell können Kombinationen aus allen in Kapitel \ref{chap:nlp} genannten Modellen erprobt werden.

Auf Anwendungsebene eröffnet sich weiteres Potenzial durch das Deployment der entwickelten Anwendung, um diese nicht nur lokal zu demonstrieren.
Durch die Integration zusätzlicher Nachrichtenportale ließe sich ein vielfältiges Spektrum des deutschsprachigen Journalismus automatisiert erfassen, 
analysieren und klassifizieren.
Ergänzend dazu könnte eine eigene Webplattform entwickelt werden, auf der Nutzer:innen Artikel posten und direkt eine Echtzeit-Klassifikation 
erhalten. Dies würde nicht nur die Nutzbarkeit verbessern, sondern auch einen konkreten gesellschaftlichen Mehrwert leisten.

Insgesamt bieten sich zahlreiche Perspektiven zur Weiterentwicklung sowohl im Hinblick auf Modellarchitektur als auch auf Anwendungsintegration. 
Die hier vorgestellte Arbeit kann somit als Ausgangspunkt für praxisnahe, hochskalierbare und transparente Systeme zur Bekämpfung von Desinformation 
im digitalen Raum dienen.

