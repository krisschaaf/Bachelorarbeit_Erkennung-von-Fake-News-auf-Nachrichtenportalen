\chapter{Natural Language Processing}
\label{chap:nlp}

\section{Machine Learning}
\label{sec:ml}

\cite{sabir2025}
\cite{Buddhadev2025}

\subsection{Textbereinigung und Vorverarbeitung}
\label{sec:text_vorverarbeitung}

\begin{itemize}
    \item \textbf{Titel und Inhalt der Artikel zusammenfügen \cite{Buddhadev2025}}: 
    \item \textbf{Akzente und Sonderzeichen entfernen \cite{Buddhadev2025} \cite{sabir2025}}
    \item \textbf{Alle Buchstaben zu Kleinbuchstaben konvertieren \cite{sabir2025} \cite{SUDHAKAR2024101028}}
    \item \textbf{Leere Spalten entfernen \cite{SUDHAKAR2024101028}}
    \item \textbf{Kontraktionen auflösen (ans -> an das) \cite{Buddhadev2025}}
    \item \textbf{Stoppwörter entfernen \cite{sabir2025} \cite{Buddhadev2025}}
    \item \textbf{Rechtschreibfehler korrigieren \cite{sabir2025}} 
    \item \textbf{Lemmatisieren \cite{Buddhadev2025} \cite{sabir2025}}
    \item \textbf{Tokenisierung \cite{sabir2025}}
\end{itemize}

\paragraph{Eine duale Feature-Pipeline} 

\subsection{Merkmalextraktion}
\label{sec:merkmalextraktion}

\subsubsection{Bag-of-words}

\subsubsection{TF-IDF}


\subsection{Machine Learning Modelle}
\label{sec:ml_modelle}

\subsubsection{Naive Bayes}

\subsubsection{Support Vector Machines}

\subsubsection{Logistische Regression}


\section{Deep Learning}
\label{sec:deep_learning}

\subsection{Word Embeddings}
\label{sec:word_embeddings}

\subsubsection{Word2Vec}

\subsubsection{GloVe}

\subsubsection{BERT-Tokenisierung}


\subsection{Deep Learning-Modelle}
\label{sec:deep_learning_modelle}

\subsubsection{CNN}

\subsubsection{LSTM}

\subsubsection{Transformer: BERT}


\section{Hybride Modelle}
\label{sec:hybride_modelle}

\subsection{convolutional neural network (CNN) along with bi-directional LSTM}

\cite{Buddhadev2025}

% \section{Diskussion: White Box vs. Black Box-Ansätze zur Erklärbarkeit}
% \label{sec:diskussion_erklaerbarkeit}